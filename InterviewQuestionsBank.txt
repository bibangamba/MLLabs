# Lesson1 — ML Basics, Data Preprocessing, Imputation, Learning (Supervised, Unsupervised)

• Scenario: You are working on a dataset that includes patient records in a healthcare
database. After initial analysis, you identify that 15% of the patient age data is missing.
The dataset is fairly large, with over 100,000 records. The age data is important for your
analysis to track disease prevalence across age groups.
• Question 1: Describe how you would handle these missing values. Discuss the
techniques you would consider and explain your choice. How would your approach
change if missing values were identified in a more critical feature, such as diagnosis
information?

• Scenario: You are tasked with cleaning a financial dataset used for forecasting stock
prices. The dataset contains some apparent outliers in the volume of trades, which
could potentially skew the predictive models. Preliminary analysis shows that these
outliers represent days with significant market news.
• Question 2: What methods would you use to detect these outliers, and how would you
decide whether to remove or adjust these data points in the dataset? Outline the
potential impacts of your decision on the forecast model's performance.

• Scenario: Imagine you are preparing a dataset for a machine learning model that
predicts real estate prices. The dataset features have varying scales and distributions,
including property size in square feet and local crime rate per 1,000 residents.
• Question 4: Would you choose to normalize or standardize these features, and why?
Provide a detailed explanation of how each process would affect the data and the
model's learning process. What might be the implications of choosing one method over
the other in terms of model performance and accuracy?

# Lesson2 — Regression, Linear Regression, Loss/Cost Functions, MeanSquaredError, RMSE,

• Scenario on Handling Multicollinearity: Imagine you are working with a dataset
intended to predict housing prices based on features like size, location, age of the
property, and proximity to amenities. During your analysis, you discover significant
multicollinearity between the size of the house and its age. Describe the steps you
would take to address this issue. Which specific techniques or metrics would you use to
confirm and mitigate multicollinearity to ensure the stability and interpretability of your
model?

• Scenario on Model Evaluation Metrics: You have developed a multiple linear regression
model to forecast quarterly sales based on advertising spend, seasonal effects, and
economic conditions. The model has an R-squared of 0.85, but your client is concerned
about the reliability of predictions. Discuss how you would use MSE and RMSE in this
scenario to evaluate model performance further. Explain the implications of these
metrics and how they might influence your recommendations for model adjustments or
client expectations.

# Lesson3 — Classification (bi & multi), Logistic Regression, softmax, categorical-crossentropy &

• Scenario: You are developing a machine learning model to classify news articles into
multiple categories such as politics, sports, technology, and entertainment. The dataset
contains thousands of articles, each labeled with one of these categories. The text data
is highly dimensional due to the vast vocabulary and the dataset includes both short and
long articles.
• Question: Discuss how you would approach building this classifier. What steps would
you take to handle the high dimensionality and variability in article length? Explain your
choice of classification algorithm considering the nature of the data and the need for
model interpretability.

• Scenario: You have developed a model to predict whether a customer will default on a loan.
The initial model, a logistic regression, performs adequately, but you believe performance can
be improved.
• Question: Describe the steps you would take to optimize this model. What alternative models
might you consider and why? How would you handle the deployment of this model in a
production environment to ensure it remains accurate over time? Discuss how you would set
up a feedback loop for continuous model improvement.

# Lesson4 — ANN, Backpropagation, perceptron, layers intro: (Input, Hidden, Output Layers), neurons,

• Question: Imagine you are tasked with designing an ANN-based system for a self-driving
car that needs to process real-time data from multiple sensors to navigate through
urban environments. Describe the architecture of the neural network you would
propose. Include details on the type of layers, the number of nodes in each layer, and
the activation functions you would use. How would you ensure that the network can
effectively handle varying lighting and weather conditions?

• Question: You are involved in creating an ANN model to assist in diagnosing diseases
from complex medical images, such as MRIs or CT scans. Outline the design of your
ANN, specifying the types of layers and activation functions that would be most suitable
for this task. How would you train your network to differentiate between very subtle
variations in medical images that indicate different stages of a disease? Describe the
loss function you would choose and the rationale behind this choice, considering the
critical nature of accurate medical diagnostics.


# Lesson5 — Convolutional Operations, Kernels/Filters, Feature Map
No sample interview/scenario questions provided

# Lesson6 — ConvolutionalNeuralNetworks, more layers: (PoolingLayer, DropoutLayer, FlattenLayer, ConvolutionalLayer, FullyConnectedLayer? )

• Consider a CNN tasked with classifying scenes into categories such as beaches, forests, and
cities. Discuss the role of pooling layers in this context. Would you choose max pooling or
average pooling, and why? How does the choice of pooling strategy affect the network's ability
to generalize from training data to new, unseen images? What are the implications of this
choice for the spatial resolution and the computational efficiency of the network?

• You are optimizing a CNN that categorizes x-ray images into normal and various types of
pathological findings. The network currently uses ReLU activation functions. However, you
notice that some neurons are becoming inactive and not learning during training—a problem
often referred to as "dying ReLU." How would you address this issue? Would you consider
switching to another activation function or modifying the network architecture? Explain your
reasoning and the expected impact on the network’s learning capability and performance.

# Lesson7 — Decision Trees and Random Forests
No sample interview/scenario questions provided

# Lesson8 — Ensemble Methods

1.High-Dimensional Data Scenario:
• Question: You are tasked with creating a predictive model for genetic data
classification, where each sample may have thousands of features. Explain how you
would apply the Random Patches and Random Subspaces methods of ensemble
learning to handle the high dimensionality. Discuss the potential benefits and
limitations of these methods in terms of feature selection, model accuracy, and
computational efficiency.

2.Stacking Ensemble Scenario:
• Question: Consider you have three different machine learning models: a Logistic
Regression classifier, a Decision Tree classifier, and a Support Vector Machine, all
performing with moderate accuracy on a text classification problem. You want to
improve the performance using a stacking ensemble approach. Detail the steps you
would take to implement this ensemble, including how you would generate training
sets for each layer, how you would choose the blender or meta-learner, and how you
would validate the performance of your ensemble model compared to the individual
classifiers.
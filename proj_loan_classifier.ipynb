{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9128f73a84d6988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:27.334605Z",
     "start_time": "2024-05-22T19:06:22.775409Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# imports we need + utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.pandas\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    # plot_confusion_matrix, plot_roc_curve\n",
    ")\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "# import imblearn as iblearn\n",
    "# import sklearn as skl\n",
    "# print(iblearn.__version__)\n",
    "# print(skl.__version__)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.float', '{:.2f}'.format)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "# import sklearn as sk\n",
    "# print(sk.__version__)\n",
    "# import imblearn as ib\n",
    "# print(ib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03024a03-c6c0-4f39-9cf2-6835fc5e5728",
   "metadata": {},
   "source": [
    "**Global Variables for metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e18f4a-0f36-45c0-bb33-85e9ef3c7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global data for metrics\n",
    "metrics_dict_val = {\n",
    "    'XGBoost': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1_score': 0},\n",
    "    'Logistic Regression': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1_score': 0},\n",
    "    'Random Forest': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1_score': 0}\n",
    "}\n",
    "\n",
    "metrics_dict_test = {\n",
    "    'XGBoost': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1_score': 0},\n",
    "    'Logistic Regression': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1_score': 0},\n",
    "    'Random Forest': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1_score': 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:28.438546Z",
     "start_time": "2024-05-22T19:06:27.339723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis code cell\n",
    "data = pd.read_csv(\"resources/lending_club_loan_two.csv\")\n",
    "\n",
    "\n",
    "def plot_3_row_histogram(df):\n",
    "    numerics_df = df.select_dtypes(include=['number'])\n",
    "    print(\"Number of numeric cols: \", len(numerics_df.columns), \" of \", len(df.columns))\n",
    "    n_rows = len(numerics_df.columns) // 3\n",
    "    n_rows += len(numerics_df.columns) % 3\n",
    "\n",
    "    # Create a figure and axes with matplotlib's subplots()\n",
    "    fig, axs = plt.subplots(n_rows, 3, figsize=(15, n_rows * 4))\n",
    "\n",
    "    for idx, col in enumerate(numerics_df.columns):\n",
    "        i = idx // 3\n",
    "        j = idx % 3\n",
    "\n",
    "        sns.histplot(numerics_df[col], ax=axs[i][j], kde=True)\n",
    "        axs[i][j].set_title(f\"Distribution of {col}\")\n",
    "\n",
    "    # Remove empty subplots\n",
    "    if len(numerics_df.columns) % 3:\n",
    "        for j in range(len(numerics_df.columns) % 3, 3):\n",
    "            fig.delaxes(axs[i][j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_label_distribution(df, label):\n",
    "    values = df[label].value_counts()\n",
    "    values.plot(kind='bar', figsize=(5, 3))\n",
    "    plt.title('Label values distribution')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(df):\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def percentage_of_missing_values(df):\n",
    "    missing_columns = df.columns[data.isna().any()].to_list()\n",
    "    missing_values = df[missing_columns].isna().mean() * 100\n",
    "    missing_values = missing_values.apply(lambda x: f'{x: .2f}%')\n",
    "\n",
    "    # missing_df = pd.DataFrame({'Column': missing_columns, 'Missing %': missing_values})\n",
    "    missing_df = pd.DataFrame({'Missing %': missing_values})\n",
    "    print(\"Percentage of missing values\")\n",
    "    display(missing_df)\n",
    "\n",
    "\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "    outliers = df[z_scores > threshold].index\n",
    "    return outliers\n",
    "\n",
    "\n",
    "def show_outliers(df):\n",
    "    outlier_data = []\n",
    "    numerics_df = df.select_dtypes(include=['number'])\n",
    "    for col in numerics_df.columns:\n",
    "        ol = detect_outliers_zscore(numerics_df, col)\n",
    "        outlier_data.append({'Column': col, 'Outlier Count': len(ol)})\n",
    "    outliers_df = pd.DataFrame(outlier_data)\n",
    "    print(\"Outliers found in each feature\")\n",
    "    display(outliers_df.sort_values(by='Outlier Count', ascending=False))\n",
    "\n",
    "\n",
    "def show_unique_values_categorical_features(df):\n",
    "    categorical_cols = data.select_dtypes(include=['object'])\n",
    "    unique_categories = []\n",
    "    for col in categorical_cols.columns:\n",
    "        unique_categories.append({'Column': col, 'Unique Count': data[col].nunique()})\n",
    "    unique_categories_df = pd.DataFrame(unique_categories)\n",
    "    print(\"Count of unique values in categorical features\")\n",
    "    display(unique_categories_df.sort_values(by='Unique Count', ascending=False))\n",
    "\n",
    "\n",
    "def feature_importance_random_forest(x, y):\n",
    "    # Train a Random Forest model\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(x, y)\n",
    "\n",
    "    # Get feature importance\n",
    "    importance = rf.feature_importances_\n",
    "\n",
    "    # Create a DataFrame to visualize importance\n",
    "    feature_importance = pd.DataFrame({'Feature': x.columns, 'Importance': importance})\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(feature_importance)\n",
    "\n",
    "\n",
    "def pub_rec(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def mort_acc(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    elif number >= 1.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return number\n",
    "\n",
    "\n",
    "def pub_rec_bankruptcies(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    elif number >= 1.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return number\n",
    "\n",
    "\n",
    "def eda():\n",
    "    #label distribution\n",
    "    plot_label_distribution(data, 'loan_status')\n",
    "\n",
    "    # feature distribution\n",
    "    plot_3_row_histogram(data)\n",
    "\n",
    "    #correlation matrix\n",
    "    plot_correlation_matrix(data)\n",
    "\n",
    "    # missing values\n",
    "    percentage_of_missing_values(data)\n",
    "\n",
    "    # outliers\n",
    "    show_outliers(data)\n",
    "\n",
    "    # show unique values for each categorical feature\n",
    "    show_unique_values_categorical_features(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead4a08-8360-4854-b21b-39644f7b377e",
   "metadata": {},
   "source": [
    "# Exploratory Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d3f3171c19c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.329501Z",
     "start_time": "2024-05-22T19:06:28.456346Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# call exploratory data analysis methods\n",
    "# eda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b0a86-d311-4f1a-bfa5-5313a8b8ecdc",
   "metadata": {},
   "source": [
    "## EDA Observations\n",
    "\n",
    "## Distributions:\n",
    "#### Right Skewed: \n",
    "* *loan_amnt*, *int_rate*, *installment*, *annual_inc*, *dti*, *open_acc*, *pub_rec*, *revol_bal*, *total_acc* \n",
    "* These variables have a long tail on the right, indicating a concentration of values at the lower end with a few very high values.\n",
    "\n",
    "#### Approximately Normal: \n",
    "* *revol_util*\n",
    "* This distribution resembles a bell curve, suggesting a symmetric distribution of values around the mean.\n",
    "\n",
    "#### Highly Imbalanced: \n",
    "* `mort_acc, pub_rec_bankruptcies` \n",
    "* These variables have a very uneven distribution, with a large majority of values concentrated in a single category.\n",
    "\n",
    "### Preprocessing Strategies:\n",
    "1. Addressing the Imbalance in label instances:\n",
    "    - Oversampling using SMOTE and ADASYN: From the bar graph above relating to Fully Paid vs Charged-Off, we can see a significant imbalance in label instance that need to be addressed.\n",
    "    - We're using both Synthetic Minority Sampling and Adaptive Synthetic Sampling to experiment and cross-validate which is better since SMOTE might not perform well with complex distributions\n",
    "2. Handling Skewness:\n",
    "    - We chose to cap the extreme values at a certain percentile to reduce the impact of these outliers without removing them entirely. \n",
    "3. Normalization/Standardization:\n",
    "    - We shall standardize `revol_util` since it's close to normal distribution. This will help improve model performance.\n",
    "    - All other variables to are going to be normalized using min-max scaling to rescale them to a range between 0 and 1. This will prevent features with larger variations from biasing the models we train later on.\n",
    "4. Because `pub_rec, mort_acc, pub_rec_bankruptcies` are heavily right-skewed and have most of their values as 0, we're choosing to simplify them into binary e.g. 0 - no mort_acc, 1 - has mort_acc  \n",
    "5. Address feature needs to be augmented/engineered into zip_code because there are too many variations for it to be encoded effeciently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16a1c9-b88c-470c-80f2-ddd2f4d7b9de",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The goal here is to do the following:\n",
    "\n",
    "* Impute missing values\n",
    "* Remove repeating and irrelevant features\n",
    "* Encode categorical features into numerical format\n",
    "* Oversample the minority class to increase its number of instances\n",
    "* Change some features into variations of themselves (feature engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641959f-a11e-4e36-8bbe-398df8b9c0f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.855812Z",
     "start_time": "2024-05-22T19:06:54.330606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert features to simple binary \n",
    "data['pub_rec'] = data.pub_rec.apply(pub_rec)\n",
    "data['mort_acc'] = data.mort_acc.apply(mort_acc)\n",
    "data['pub_rec_bankruptcies'] = data.pub_rec_bankruptcies.apply(pub_rec_bankruptcies)\n",
    "data['loan_status'] = data.loan_status.map({'Fully Paid': 1, 'Charged Off': 0})\n",
    "\n",
    "# one-hot encode categories we found as categorical\n",
    "one_hot_encode_cols = ['verification_status',  'initial_list_status',\n",
    "                       'application_type']\n",
    "data = pd.get_dummies(data, columns=one_hot_encode_cols, drop_first=True)\n",
    "\n",
    "# the shape of the data\n",
    "print(f\"The shape of the dataset (instances, features): {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335ac60-80dd-4340-84fc-ac962d7bdc75",
   "metadata": {},
   "source": [
    "### 2.2 Converting categorical string features into numerical formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ecb18-56f9-47da-9c4d-0390c2216dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.860391Z",
     "start_time": "2024-05-22T19:06:54.851943Z"
    }
   },
   "outputs": [],
   "source": [
    "data.term.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc30b3d-e077-42e3-b4dc-308a0a5f052f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.892739Z",
     "start_time": "2024-05-22T19:06:54.871804Z"
    }
   },
   "outputs": [],
   "source": [
    "term_values = {' 36 months': 36, ' 60 months': 60}\n",
    "data['term'] = data.term.map(term_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a988aa2-1903-4df7-aa80-81d54f0d5789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.894346Z",
     "start_time": "2024-05-22T19:06:54.875911Z"
    }
   },
   "outputs": [],
   "source": [
    "data.term.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd4bc2-0bb8-483c-890f-1daf7e28aad4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.924041Z",
     "start_time": "2024-05-22T19:06:54.881509Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop('grade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e645a-fec0-427d-bd2a-2315d657eee1",
   "metadata": {},
   "source": [
    "address\n",
    "We are going to feature engineer a zip code column from the address in the data set. Create a column called 'zip_code' that extracts the zip code from the address column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669d70c-129a-4b3b-8bc7-1bdb0a743173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.925198Z",
     "start_time": "2024-05-22T19:06:54.918756Z"
    }
   },
   "outputs": [],
   "source": [
    "data.address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f300d9-fce6-48cc-af94-6edb2463f28a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.986330Z",
     "start_time": "2024-05-22T19:06:54.943278Z"
    }
   },
   "outputs": [],
   "source": [
    "data['zip_code'] = data.address.apply(lambda x: x[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a24e7-a083-4a7d-b491-d07015f271b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:54.999329Z",
     "start_time": "2024-05-22T19:06:54.996264Z"
    }
   },
   "outputs": [],
   "source": [
    "data.zip_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40897860-a44f-4eb7-8819-7f6811eb9989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:55.131904Z",
     "start_time": "2024-05-22T19:06:55.034449Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.get_dummies(data, columns=['zip_code'], drop_first=True)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the zip_code column\n",
    "data['zip_code_encoded'] = label_encoder.fit_transform(data['zip_code'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['home_ownership_encoded'] = label_encoder.fit_transform(data['home_ownership'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['purpose_encoded'] = label_encoder.fit_transform(data['purpose'])\n",
    "\n",
    "data.drop('zip_code', axis=1, inplace=True)\n",
    "data.drop('purpose', axis=1, inplace=True)\n",
    "data.drop('home_ownership', axis=1, inplace=True)\n",
    "\n",
    "data.drop('address', axis=1, inplace=True)\n",
    "\n",
    "sub_grade_mapping = {\n",
    "    'A1': 35, 'A2': 34, 'A3': 33, 'A4': 32, 'A5': 31,\n",
    "    'B1': 30, 'B2': 29, 'B3': 28, 'B4': 27, 'B5': 26,\n",
    "    'C1': 25, 'C2': 24, 'C3': 23, 'C4': 22, 'C5': 21,\n",
    "    'D1': 20, 'D2': 19, 'D3': 18, 'D4': 17, 'D5': 16,\n",
    "    'E1': 15, 'E2': 14, 'E3': 13, 'E4': 12, 'E5': 11,\n",
    "    'F1': 10, 'F2': 9, 'F3': 8, 'F4': 7, 'F5': 6,\n",
    "    'G1': 5, 'G2': 4, 'G3': 3, 'G4': 2, 'G5': 1\n",
    "}\n",
    "data['sub_grade_encoded'] = data['sub_grade'].map(sub_grade_mapping)\n",
    "data.drop('sub_grade', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ef7fc-e2b6-43a9-bdae-e6b7f25238ee",
   "metadata": {},
   "source": [
    "Removing `issue_d` feature\n",
    "This would be data leakage because we would not know the loan issue date beforehand when using our model/in our usecase. Therefore choosing to drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4cecd-fb90-448d-8523-ce796b922fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:55.166073Z",
     "start_time": "2024-05-22T19:06:55.132557Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop('issue_d', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47d4eb-3ca0-4c23-947f-56b937c800d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.463975Z",
     "start_time": "2024-05-22T19:06:55.195789Z"
    }
   },
   "outputs": [],
   "source": [
    "data['earliest_cr_line'] = data['earliest_cr_line'].apply(lambda x: datetime.strptime(x, \"%b-%Y\").year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5f193-e11d-42df-8fb6-abbe3a182830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.471537Z",
     "start_time": "2024-05-22T19:06:56.464744Z"
    }
   },
   "outputs": [],
   "source": [
    "data.earliest_cr_line.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eff0a7-65c8-496f-a1dc-5275e5a80699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.474418Z",
     "start_time": "2024-05-22T19:06:56.471819Z"
    }
   },
   "outputs": [],
   "source": [
    "data.earliest_cr_line.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b983ded-fb7f-4f5f-93a2-3ecd3a12ed1e",
   "metadata": {},
   "source": [
    "### Removing or Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf78381-e948-40c4-82c8-4f62d54c5856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.531795Z",
     "start_time": "2024-05-22T19:06:56.474970Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].isna().sum() != 0:\n",
    "        missing = data[column].isna().sum()\n",
    "        portion = (missing / data[column].shape[0]) * 100\n",
    "        print(f\"'{column}' number of missing values: '{missing}' ==> '{portion:.3f}%'\")\n",
    "        # print(\n",
    "        #     f\"{bold_start}'{column}'{bold_end}: number of missing values: {bold_start}'{missing}'{bold_end} ==> {bold_start}'{portion:.3f}%'{bold_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c1875-836a-461f-9f38-021abceb2d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.570068Z",
     "start_time": "2024-05-22T19:06:56.557973Z"
    }
   },
   "outputs": [],
   "source": [
    "data['emp_title'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e99677-9109-4431-9aa9-e4a87a6bf834",
   "metadata": {},
   "source": [
    "The **emp_title** has too many unique values to encode and dataset will not be sufficent if used with it. So, the best thing to do is drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d5354-3710-4a7a-82c6-d910dfac33ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.600839Z",
     "start_time": "2024-05-22T19:06:56.563489Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop('emp_title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d215aa-3b68-4c3d-98eb-258e13cb74d1",
   "metadata": {},
   "source": [
    "Now, let's look at **emp_length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17f680-dd22-4647-9697-135689806955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.602662Z",
     "start_time": "2024-05-22T19:06:56.597612Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"emp_length\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bed94-d39c-462f-97c3-4f4fff2289d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.864280Z",
     "start_time": "2024-05-22T19:06:56.613523Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in data[\"emp_length\"].unique():\n",
    "    print(f\"{year} years in the positions:\")\n",
    "    # print(f\"{bold_start}{year}{bold_end} years in the positions:\")\n",
    "    print(f\"{data[data.emp_length == year].loan_status.value_counts(normalize=True)}\")\n",
    "    print(\"\\n=============================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec114452-b2ab-4275-b017-cf06c8aada19",
   "metadata": {},
   "source": [
    "**Charge offs** seem to be extremely similar across all **emp_length**. So, we are going to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e7f05-d13c-4c41-aa83-97975fafa571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.876823Z",
     "start_time": "2024-05-22T19:06:56.858308Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop('emp_length', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828564bb-295f-4c74-8b02-d9fd20acde49",
   "metadata": {},
   "source": [
    "Now, let's have a look at **title**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477df7b-bd84-4d43-9cbc-089c600dfa3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.914186Z",
     "start_time": "2024-05-22T19:06:56.898190Z"
    }
   },
   "outputs": [],
   "source": [
    "data.title.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff63932-5276-4c67-8e5a-3786080c56a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.935691Z",
     "start_time": "2024-05-22T19:06:56.921788Z"
    }
   },
   "outputs": [],
   "source": [
    "data.title.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4a58b-dcfe-465b-8cef-b39d7ad2a49b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.965578Z",
     "start_time": "2024-05-22T19:06:56.925630Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(\"title\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f54cba-03c9-4756-9230-c97729437f6a",
   "metadata": {},
   "source": [
    "Let's have a look at **mort_acc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155bc8e-ba20-4456-a002-19d6ef61dcec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.967160Z",
     "start_time": "2024-05-22T19:06:56.949568Z"
    }
   },
   "outputs": [],
   "source": [
    "data.mort_acc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cf872-c297-4114-bbf5-7b5f4c5e8d4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:06:56.968737Z",
     "start_time": "2024-05-22T19:06:56.952678Z"
    }
   },
   "outputs": [],
   "source": [
    "data.mort_acc.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c994ec6-bf3f-430f-8597-42c69e8f702e",
   "metadata": {},
   "source": [
    "Imputing **mort_acc** should be imputed using either mean or prediction model, for this purpose we will do using **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd378b-3bcf-4444-a96f-a021b5d07705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:07:53.496659Z",
     "start_time": "2024-05-22T19:06:56.956845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into two parts: one with missing values and one without\n",
    "missing_data = data[data['mort_acc'].isnull()]\n",
    "complete_data = data.dropna(subset=['mort_acc'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = complete_data.drop(columns=['mort_acc'])\n",
    "y = complete_data['mort_acc']\n",
    "\n",
    "# Split the complete data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing values\n",
    "imputed_values = model.predict(missing_data.drop(columns=['mort_acc']))\n",
    "\n",
    "# Impute missing values in the mort_acc column\n",
    "data.loc[data['mort_acc'].isnull(), 'mort_acc'] = imputed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407fd0a-672c-46a8-bcee-64c7755519af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:07:53.507002Z",
     "start_time": "2024-05-22T19:07:53.495681Z"
    }
   },
   "outputs": [],
   "source": [
    "data.mort_acc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4cb7e-18c3-4d92-b98f-9bab6b843ba9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d43ac-6d7f-4c5d-8fdc-1a3130bf7559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:07:53.585303Z",
     "start_time": "2024-05-22T19:07:53.505275Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].isna().sum() != 0:\n",
    "        missing = data[column].isna().sum()\n",
    "        portion = (missing / data.shape[0]) * 100\n",
    "        print(f\"'{column}': number of missing values '{missing}' ==> '{portion:.3f}%'\")\n",
    "\n",
    "# drop missing value instances that are less than 0.2% of the dataset\n",
    "data.dropna(inplace=True)\n",
    "display(data.shape)\n",
    "data.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8850781f2f6dd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Removing `revol_util & pub_rec_bankruptcies` missing values instances\n",
    "These two features have missing data points, but they account for less than 0.5% of the total data. So we are going to remove the rows that are missing those values in those columns with dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f12641-f9eb-4b79-8c49-8164b7ff64ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:07:53.647217Z",
     "start_time": "2024-05-22T19:07:53.581138Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06be12-9661-4601-b4aa-2d1cdc0dec9c",
   "metadata": {},
   "source": [
    "## Preparing the Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38872c11-da77-4078-a9cd-2fef1da3936f",
   "metadata": {},
   "source": [
    "### Checking distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf9f52-5e63-4efb-b35e-94ebb8024914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:07:53.862668Z",
     "start_time": "2024-05-22T19:07:53.619225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a threshold for Z-score\n",
    "threshold = 3\n",
    "\n",
    "# Create a copy of the data to avoid modifying the original DataFrame\n",
    "data_clean = data.copy()\n",
    "\n",
    "# Dictionary to store counts of outliers for each column\n",
    "outliers_count_dict = {}\n",
    "\n",
    "# Loop through each numerical column in the DataFrame\n",
    "for column in data_clean.select_dtypes(include=[np.number]).columns:\n",
    "    # Calculate Z-scores\n",
    "    z_scores = (data_clean[column] - data_clean[column].mean()) / data_clean[column].std()\n",
    "    # Identify outliers\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    # Store the count of previous outliers\n",
    "    outliers_count_dict[column] = outliers.sum()\n",
    "    # Calculate the mean value of the column\n",
    "    mean_value = data_clean[column].mean()\n",
    "    # If the column is of type int64, cast the mean value to int\n",
    "    if pd.api.types.is_integer_dtype(data_clean[column]):\n",
    "        mean_value = int(mean_value)\n",
    "    # Replace outliers with the mean value of the column\n",
    "    data_clean.loc[outliers, column] = mean_value\n",
    "\n",
    "# Print the shape of the original and cleaned data\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Cleaned data shape: {data_clean.shape}\")\n",
    "\n",
    "# Print count of previous outliers\n",
    "print(\"\\nCount of previous outliers in each column:\")\n",
    "for column, count in outliers_count_dict.items():\n",
    "    print(f\"Column: {column} - Outliers: {count}\")\n",
    "\n",
    "# Optional: Check if there are any remaining outliers\n",
    "remaining_outliers = ((data_clean - data_clean.mean()) / data_clean.std()).abs() > threshold\n",
    "print(\"\\nRemaining outliers in each column after cleaning:\")\n",
    "print(remaining_outliers.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344da8236d6a5d9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Oversampling the dataset using SMOTE and ADASYN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e41b7-dbf8-44bf-a8d4-ce97ef87fb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T19:08:12.717614Z",
     "start_time": "2024-05-22T19:07:53.865898Z"
    }
   },
   "outputs": [],
   "source": [
    "def smote_oversampling(x, y):\n",
    "    \"\"\"\n",
    "    Synthetic Minority Sampling\n",
    "    :param x: predictors dataframe\n",
    "    :param y: label dataframe\n",
    "    :return: x, y dataframes with synthetic instances\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    # fit and resample the data\n",
    "    x_resampled, y_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "    return x_resampled, y_resampled\n",
    "\n",
    "\n",
    "def adasyn_oversampling(x, y):\n",
    "    \"\"\"\n",
    "    Adaptive Synthetic Sampling\n",
    "    :param x: predictors dataframe\n",
    "    :param y: label dataframe\n",
    "    :return: x, y dataframes with synthetic instances\n",
    "    \"\"\"\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    # fit and resample the data\n",
    "    x_resampled, y_resampled = adasyn.fit_resample(x, y)\n",
    "\n",
    "    return x_resampled, y_resampled\n",
    "\n",
    "\n",
    "# Assuming data_clean is your DataFrame and 'target' is the target variable\n",
    "X = data_clean.drop(columns=['loan_status'])\n",
    "y = data_clean['loan_status']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "smote_x, smote_y = smote_oversampling(X_train, y_train)\n",
    "adasyn_x, adasyn_y = adasyn_oversampling(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f\"Original dataset shape: {Counter(y_train)}\")\n",
    "print(f\"Resampled dataset shape: {Counter(y_train_res)}\")\n",
    "print(f\"SMOTE sampled dataset shape: {Counter(smote_y)}\")\n",
    "print(f\"ADASYN sampled dataset shape: {Counter(adasyn_y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa46043b090cf98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba685998-e558-4c97-b93f-b03f676fcfd0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T19:08:12.719634Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# print(\"Feature importance on SMOTE dataset\")\n",
    "# feature_importance_random_forest(smote_x, smote_y)\n",
    "\n",
    "# print(\"Feature importance on ADASYN dataset\")\n",
    "# feature_importance_random_forest(adasyn_x, adasyn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c548992-5f56-4053-b5de-e50417216e1f",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a79b7-5f58-4790-adfe-25c159e2acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(smote_x, smote_y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "# model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_class_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fd967-86b2-4e32-95fd-818ee67fa9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b6397-7f38-4451-9e18-7ca927f1c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1_score = f1_score(y_val, y_val_pred)\n",
    "\n",
    "# Fill metrics for validation set\n",
    "metrics_dict_val['Logistic Regression'] = {'accuracy': val_accuracy, 'precision': val_precision, 'recall': val_recall, 'f1_score': val_f1_score}\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1_score = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Fill metrics for test set\n",
    "metrics_dict_test['Logistic Regression'] = {'accuracy': test_accuracy, 'precision': test_precision, 'recall': test_recall, 'f1_score': test_f1_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214bd63-a071-4663-9fcd-ddd41a210693",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cba897-cfa1-4538-8561-e67565d8013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# # Define the model\n",
    "# input_dim = smote_x.shape[1]\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, input_dim=input_dim))\n",
    "# model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
    "# model.add(Dense(32))\n",
    "# model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
    "# model.add(Dense(16))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model with class weights\n",
    "# model.fit(smote_x, smote_y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf6c21-5b2b-4f52-b609-a06970b45a31",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ff34a-aa60-4753-8273-3e26ba83d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(smote_x, label=smote_y)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define the parameter dictionary\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # binary classification\n",
    "    'max_depth': 10,  # maximum depth of a tree\n",
    "    'eta': 0.03,  # step size shrinkage\n",
    "    'eval_metric': 'logloss',  # evaluation metric\n",
    "    'random_state': 42  # seed for reproducibility\n",
    "}\n",
    "\n",
    "# Specify the training and validation sets\n",
    "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=1000, evals=evals, early_stopping_rounds=10)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "y_val_pred = (bst.predict(dval) > 0.5).astype(int)\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "precision_val = precision_score(y_val, y_val_pred)\n",
    "recall_val = recall_score(y_val, y_val_pred)\n",
    "f1_score_val = f1_score(y_val, y_val_pred)\n",
    "\n",
    "# Fill metrics for validation set\n",
    "metrics_dict_val['XGBoost'] = {'accuracy': accuracy_val, 'precision': precision_val, 'recall': recall_val, 'f1_score': f1_score_val}\n",
    "\n",
    "# Calculate metrics for test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "f1_score_test = f1_score(y_test, y_pred)\n",
    "\n",
    "# Fill metrics for test set\n",
    "metrics_dict_test['XGBoost'] = {'accuracy': accuracy_test, 'precision': precision_test, 'recall': recall_test, 'f1_score': f1_score_test}\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8799c-48c4-4a21-8160-29f676fb76da",
   "metadata": {},
   "source": [
    "## 6. Evaluation of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dda97b-445b-4f90-9a22-7e9ddf2116b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the metrics\n",
    "models = list(metrics_dict_val.keys())\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# Create 4 subplots (one for each metric) arranged in a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = list(metrics_dict_val[models[0]].keys())  # Metrics names\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    values_val = [metrics_dict_val[model][metric] for model in models]\n",
    "    values_test = [metrics_dict_test[model][metric] for model in models]\n",
    "\n",
    "    axs[row, col].bar(x - width / 2, values_val, width, label='Validation', color='b')\n",
    "    axs[row, col].bar(x + width / 2, values_test, width, label='Test', color='g')\n",
    "\n",
    "    axs[row, col].set_ylabel(metric.capitalize())\n",
    "    axs[row, col].set_title(f'{metric.capitalize()} Comparison: Validation vs Test Set')\n",
    "    axs[row, col].set_xticks(x)\n",
    "    axs[row, col].set_xticklabels(models)\n",
    "    axs[row, col].legend(loc='center right')\n",
    "\n",
    "    # Attach a text label above each bar in rects, displaying its height.\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            axs[row, col].annotate(f'{height:.2f}',\n",
    "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "\n",
    "    autolabel(axs[row, col].bar(x - width / 2, values_val, width, label='Validation', color='b'))\n",
    "    autolabel(axs[row, col].bar(x + width / 2, values_test, width, label='Test', color='g'))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
